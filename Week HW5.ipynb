{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a4f18e",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "Ideas that can be tested must be supported by data and ideas that cannot be tested are generally people's subjective ideas that cannot be supported by data. A good null hypothesis is that something can be tested and there is a lot of data to support the original hypothesis, which has no effect or no significant difference between groups The alternative hypothesis is to assume something happened, and because they are opposites, we can only accept one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000400e9",
   "metadata": {},
   "source": [
    "# Q2\n",
    "\n",
    "\n",
    "When we do a statistical test, we collect data from a small group which is a sample to make guesses about a larger group(population). For example, if we want to collect the data of average mark of English class, we will collect the whole lecture parts instead of one small tutorial. The numbers we calculate from the sample are called sample statistics, but we are care about is the true value for the whole population which is the population parameter. This can help us understand the whole group not only the sample.\n",
    "\n",
    "\n",
    "Chatgpt link: https://chatgpt.com/share/670f2703-bf14-8009-b73f-813035d1cfc2\n",
    "\n",
    "Summary:\n",
    "\n",
    "In hypothesis testing, we use data from a sample (a small group) to make conclusions about a population (the whole group). The results of a test are focused on what we can learn about the population parameter (the true value for the whole population) based on the sample statistic (the number calculated from the sample). The key distinction is that we perform these tests to understand the population, even though we only use a small part of it (the sample) to draw conclusions. The purpose of hypothesis testing is to evaluate whether there is enough evidence to reject the null hypothesis (which usually states there is no effect or difference) in favor of the alternative hypothesis (which suggests there is an effect or difference). The conclusions are always about the population, not just the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d713d",
   "metadata": {},
   "source": [
    "# Q3\n",
    "\n",
    "When calculating the p-value, we begin by assuming that the null hypothesis(Ho) is true, meaning there is no effect or difference between the groups. So we then check if the p-value is extremely small, which would indicate that the null hypothesis is likely wrong. In that case, we reject Hâ‚€ and consider the alternative hypothesis(Ha). It's difficult to directly prove that Ha is correct so we test H0 to see if there is evidence strongly rejecting it. The two hypotheses are opposites of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d21b1",
   "metadata": {},
   "source": [
    "# Q4\n",
    "\n",
    "A small p-value shows that the evidence from our data is very ridiculous so we start to doubt the null hypothesis. For example, from the video, if the dog is innocent in this case, the garbage cover on his head is done by someone else but we know that the probability of the neighborhood kid coming to our house and putting a garbage bag over his head is very small so it can only be done by the dog itself so we reject H0 and p value is very small, which is very ridiculous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "474d1025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated p-value: 0.0007\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_simulations = 10000\n",
    "n_trials = 124\n",
    "observed_right_tilts = 80\n",
    "\n",
    "p_null = 0.5\n",
    "\n",
    "simulations = np.random.binomial(n=n_trials, p=p_null, size=n_simulations)\n",
    "\n",
    "p_value = np.mean(simulations >= observed_right_tilts)\n",
    "\n",
    "print(f\"Simulated p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e93a084",
   "metadata": {},
   "source": [
    "P-value is smaller than 0.001, therefore there is very strong evidence against the null hypothesis, which is reject H0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2896d0a",
   "metadata": {},
   "source": [
    "# Q6\n",
    "\n",
    "It can't prove that is false because we should use the p-value table which is decision rule to see each intervals instead of a point. Even if our evidence is very strong, the answer is not 100% correct. Because there's also error for type1 and type2. Type1 error means when we reject the null hypothesis H0, when it is actually true. Type2 error means we fail to reject the null hypothesis when the alternative hypothesis Ha is actually true. Therefore, although smaller p-value could has less probability to make mistakes but it is still wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212e2f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Mean: 3.3\n",
      "95% Confidence Interval (Two-tailed): [0.9 5.5]\n",
      "One-tailed p-value (Mean Health Score Change > 0): 0.5278\n"
     ]
    }
   ],
   "source": [
    "# Q7\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1500)\n",
    "\n",
    "data = {\n",
    "    \"PatientID\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"InitialHealthScore\": [84, 78, 83, 81, 81, 80, 79, 85, 76, 83],\n",
    "    \"FinalHealthScore\": [86, 86, 80, 86, 84, 86, 86, 82, 83, 84]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate change in health scores\n",
    "df['HealthScoreChange'] = df['FinalHealthScore'] - df['InitialHealthScore']\n",
    "\n",
    "# Bootstrap sampling\n",
    "n_iterations = 10000\n",
    "boot_means = []\n",
    "observed_mean = df['HealthScoreChange'].mean()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    boot_sample = df['HealthScoreChange'].sample(frac=1, replace=True)\n",
    "    boot_means.append(boot_sample.mean())\n",
    "\n",
    "# Confidence interval for two-tailed test\n",
    "confidence_interval = np.percentile(boot_means, [2.5, 97.5])\n",
    "\n",
    "# For the one-tailed test (checking if the mean change is greater than 0)\n",
    "# Calculate p-value for one-tailed test (greater than 0)\n",
    "p_value_one_tailed = np.sum(np.array(boot_means) >= observed_mean) / n_iterations\n",
    "\n",
    "print(f\"Observed Mean: {observed_mean}\")\n",
    "print(f\"95% Confidence Interval (Two-tailed): {confidence_interval}\")\n",
    "print(f\"One-tailed p-value (Mean Health Score Change > 0): {p_value_one_tailed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74fbbc",
   "metadata": {},
   "source": [
    "# Q7(Continued)\n",
    "\n",
    "\n",
    "In the original code, a two-tailed p-value checks how extreme the observed mean is in both directions (positive and negative). For a one-tailed test, we only care about one side, specifically whether the mean is greater than 0. To do this, we compare the observed mean to the simulated (bootstrap) distribution and count how many of the simulated results are greater than or equal to the observed mean.\n",
    "\n",
    "For the one-tailed test, the p-value is usually smaller because we are only looking at one side of the distribution (greater than or less than 0), while a two-tailed test splits the significance level between both sides, making it harder to detect a result in just one direction.\n",
    "\n",
    "Chatgpt link: https://chatgpt.com/share/670f2703-bf14-8009-b73f-813035d1cfc2\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "In the previous discussion, we examined how to modify a two-tailed hypothesis test to a one-tailed test using code. The key changes involved:\n",
    "\n",
    "Shifting focus from checking for a general difference in both directions (two-tailed) to checking for a specific directional difference (one-tailed).\n",
    "Adjusting the p-value calculation to reflect this focus, where we only consider values greater than or less than the observed mean, depending on the hypothesis.\n",
    "In a one-tailed test, the interpretation changes because you're only testing one direction (e.g., whether the mean is greater than 0). This makes the test more sensitive and typically results in a smaller p-value compared to the two-tailed test, where the alpha is split between both tails of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57c37a",
   "metadata": {},
   "source": [
    "# Q8\n",
    "\n",
    "The students are guessing, meaning they have a 50% chance of correctly identifying whether tea or milk was poured first. \n",
    "H0: p = 0.5, where p is the proportion of students who correctly identify the order.\n",
    "\n",
    "The students are better than random guessing in identifying whether tea or milk was poured first.\n",
    "Ha: p >= 0.5\n",
    "\n",
    "The observed proportion of correct answers is p head = 49/80 = 0.6125."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f80039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated p-value: 0.0294\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_students = 80  # Total number of students\n",
    "n_correct = 49  # Number of students who correctly identified the order\n",
    "p_null = 0.5  # Null hypothesis proportion (50% guessing)\n",
    "n_simulations = 10000  # Number of simulations\n",
    "\n",
    "# Simulate 10,000 experiments under the null hypothesis (50% guessing)\n",
    "simulated_results = np.random.binomial(n=n_students, p=p_null, size=n_simulations)\n",
    "\n",
    "# Calculate p-value: proportion of simulations where number of correct answers >= 49\n",
    "p_value = np.mean(simulated_results >= n_correct)\n",
    "\n",
    "# Output p-value\n",
    "print(f\"Simulated p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde78c8",
   "metadata": {},
   "source": [
    "Since the p-value (0.0294) is less than the common significance level of 0.05 and greater than 0.01, we have moderate evidence to reject the null hypothesis. This suggests that the STA130 students may indeed be better than random guessing at identifying whether the milk or tea was poured first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783093ce",
   "metadata": {},
   "source": [
    "# Q9\n",
    "\n",
    "Yes, I have reviewed the course wiki-textbook, which has been helpful in clarifying many concepts. Iâ€™ve also interacted with a ChatBot to address some of the material that I didnâ€™t fully grasp at first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
